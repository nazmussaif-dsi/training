{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazmussaif-dsi/training/blob/master/Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HSn4NT9OgJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/CNN/yolov3_from_scratch/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmCSPOv6h2v6",
        "colab_type": "text"
      },
      "source": [
        "###Here you need to load darknet & util modules  in addition to the pallete file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1qykQy3Edqu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Installing PyTorch \n",
        "!pip install torch==0.4\n",
        "# !wget https://raw.githubusercontent.com/AvivSham/YOLO_V3_from_scratch_colab/master/util.py\n",
        "# !wget https://raw.githubusercontent.com/AvivSham/YOLO_V3_from_scratch_colab/master/darknet.py\n",
        "# !wget https://raw.githubusercontent.com/AvivSham/YOLO_V3_from_scratch_colab/master/pallete\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc4iHPOPEBeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Import dependencies\n",
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "from util import *\n",
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "from darknet import *\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqiOI837xpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Get weights and graph file for the model\n",
        "\n",
        "# #Get weights file for the model\n",
        "# !wget https://pjreddie.com/media/files/yolov3.weights\n",
        "# # Get YOLO graph file\n",
        "# !wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBV3Wr6UUchD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Making directories for test photos\n",
        "# !mkdir Images\n",
        "# !mkdir Images_Detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLtGYrrQ9RUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Changing working directory\n",
        "# os.chdir(os.getcwd()+'/'+'Images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_12kN_K9LgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Test\n",
        "# os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeiiiiZv7ucK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Get test images\n",
        "# !wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/giraffe.jpg\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/messi.jpg\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/img1.jpg\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/img2.jpg\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/img3.jpg\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/imgs/img4.jpg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-uPUinE9qIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd1xmkVMLyqd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Making directory for COCO names file\n",
        "# !mkdir data\n",
        "# os.chdir('./data')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXiVsY_tPb0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title COCO names file\n",
        "# !wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ4naucbKBna",
        "colab_type": "code",
        "outputId": "1fa7e626-7d7b-425c-b420-c970578382f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#@title Setting parameters\n",
        "\n",
        "# images --> Input image / directory of images.\n",
        "# reso   --> Input image resolution\n",
        "# detect_image --> flag to decide to detect video or single photo\n",
        "\n",
        "images = os.getcwd() + '/Images'\n",
        "images_det = os.getcwd() + '/Images_Detection'\n",
        "print(images, images_det)\n",
        "batch_size = 1\n",
        "confidence = 0.5\n",
        "nms_thesh = 0.4\n",
        "reso = 640\n",
        "weights_file = \"yolov3.weights\"\n",
        "config_file = \"yolov3.cfg\"\n",
        "start = 0\n",
        "detect_image = False\n",
        "CUDA = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/CNN/yolov3_from_scratch/Images /content/drive/My Drive/Colab Notebooks/CNN/yolov3_from_scratch/Images_Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqLOznoKP9p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_classes(namefile):\n",
        "  fp = open(namefile, 'r')\n",
        "  names = fp.read().split(\"\\n\")[:-1] #discard the last\n",
        "  return names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOLR4HS3PiLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COCO dataset has 80 different classes\n",
        "classes = load_classes(\"data/coco.names\")\n",
        "num_classes = len(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPwDa-8HYTDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_video_box(x, results):\n",
        "  cls = int(x[-1])\n",
        "  if classes[cls] == 'person':\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    img = results\n",
        "    color = random.choice(colors)\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    cv2.rectangle(img, c1, c2,color, 3)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2,color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHdbblKGQc6m",
        "colab_type": "code",
        "outputId": "a059743e-4b35-4d66-9d27-0f49bcd4b151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "# Set up the network\n",
        "print(\"Loading network...\")\n",
        "model = Darknet(config_file)\n",
        "model.load_weights(weights_file)\n",
        "print(\"Network successfully loaded\")\n",
        "\n",
        "model.net_info[\"height\"] = reso\n",
        "inp_dim = int(model.net_info[\"height\"])\n",
        "assert inp_dim % 32 == 0\n",
        "assert inp_dim > 32\n",
        "\n",
        "# if GPU is available allocate the model\n",
        "if CUDA:\n",
        "  model.cuda()\n",
        "  \n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "if detect_image:\n",
        "  read_dir = time.time()\n",
        "  #Detection phase\n",
        "  try:\n",
        "      imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
        "  except NotADirectoryError:\n",
        "      imlist = []\n",
        "      imlist.append(osp.join(osp.realpath('.'), images))\n",
        "  except FileNotFoundError:\n",
        "      print (\"No file or directory with the name {}\".format(images))\n",
        "      exit()\n",
        "\n",
        "  if not os.path.exists(images_det):\n",
        "      os.makedirs(images_det)\n",
        "\n",
        "  load_batch = time.time()\n",
        "  loaded_ims = [cv2.imread(x) for x in imlist]\n",
        "\n",
        "  im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
        "  im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
        "  im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
        "\n",
        "  leftover = 0\n",
        "  if (len(im_dim_list) % batch_size):\n",
        "      leftover = 1\n",
        "\n",
        "  if batch_size != 1:\n",
        "      num_batches = len(imlist) // batch_size + leftover            \n",
        "      im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
        "                          len(im_batches))]))  for i in range(num_batches)]  \n",
        "\n",
        "  write = 0\n",
        "\n",
        "\n",
        "  if CUDA:\n",
        "      im_dim_list = im_dim_list.cuda()\n",
        "\n",
        "  start_det_loop = time.time()\n",
        "  for i, batch in enumerate(im_batches):\n",
        "  #load the image \n",
        "      start = time.time()\n",
        "      if CUDA:\n",
        "          batch = batch.cuda()\n",
        "      with torch.no_grad():\n",
        "          prediction = model(Variable(batch), CUDA)\n",
        "\n",
        "      prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
        "\n",
        "      end = time.time()\n",
        "\n",
        "      if type(prediction) == int:\n",
        "\n",
        "          for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
        "              im_id = i*batch_size + im_num\n",
        "              print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
        "              print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
        "              print(\"----------------------------------------------------------\")\n",
        "          continue\n",
        "\n",
        "      prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
        "\n",
        "      if not write:                      #If we have't initialised output\n",
        "          output = prediction  \n",
        "          write = 1\n",
        "      else:\n",
        "          output = torch.cat((output,prediction))\n",
        "\n",
        "      for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
        "          im_id = i*batch_size + im_num\n",
        "          objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
        "          print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
        "          print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
        "          print(\"----------------------------------------------------------\")\n",
        "\n",
        "      if CUDA:\n",
        "          torch.cuda.synchronize()       \n",
        "  try:\n",
        "      output\n",
        "  except NameError:\n",
        "      print (\"No detections were made\")\n",
        "      exit()\n",
        "\n",
        "  im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
        "\n",
        "  scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
        "\n",
        "\n",
        "  output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
        "  output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
        "\n",
        "\n",
        "\n",
        "  output[:,1:5] /= scaling_factor\n",
        "\n",
        "  for i in range(output.shape[0]):\n",
        "      output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
        "      output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
        "\n",
        "\n",
        "  output_recast = time.time()\n",
        "  class_load = time.time()\n",
        "  colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "\n",
        "  draw = time.time()\n",
        "  \n",
        "  list(map(lambda x: write_box(x, loaded_ims), output))\n",
        "\n",
        "  det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(images_det,x.split(\"/\")[-1]))\n",
        "\n",
        "  list(map(cv2.imwrite, det_names, loaded_ims))\n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\"SUMMARY\")\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
        "  print()\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
        "  print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
        "  print(\"----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "else:\n",
        "  #Detection phase\n",
        "  #Use cv2.VideoCapture('your video file name or path')\n",
        "  cap = cv2.VideoCapture('video/people.mp4')\n",
        "#   cap = cv.VideoCapture(0)\n",
        "  \n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  sz = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "       int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "  vout = cv2.VideoWriter()\n",
        "  vout.open('video/video_output_person_3.avi', fourcc, fps, sz, True)\n",
        "\n",
        "  \n",
        "  assert cap.isOpened(), 'Cannot capture source'\n",
        "\n",
        "  frames = 0 \n",
        "  start = time.time()\n",
        "\n",
        "  while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if ret:   \n",
        "          img = prep_image(frame, inp_dim)\n",
        "          im_dim = frame.shape[1], frame.shape[0]\n",
        "          im_dim = torch.FloatTensor(im_dim).repeat(1,2)   \n",
        "\n",
        "          if CUDA:\n",
        "              im_dim = im_dim.cuda()\n",
        "              img = img.cuda()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              output = model(Variable(img, volatile = True), CUDA)\n",
        "          output = write_results(output, confidence, num_classes, nms_conf = nms_thesh)\n",
        "\n",
        "          if type(output) == int:\n",
        "              frames += 1\n",
        "              key = cv2.waitKey(1)\n",
        "              if key & 0xFF == ord('q'):\n",
        "                  break\n",
        "              continue\n",
        "\n",
        "          im_dim = im_dim.repeat(output.size(0), 1)\n",
        "          scaling_factor = torch.min(416/im_dim,1)[0].view(-1,1)\n",
        "\n",
        "          output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
        "          output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
        "\n",
        "          output[:,1:5] /= scaling_factor\n",
        "\n",
        "          for i in range(output.shape[0]):\n",
        "              output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
        "              output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
        "\n",
        "\n",
        "          classes = load_classes('data/coco.names')\n",
        "          colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "\n",
        "          list(map(lambda x: write_video_box(x, frame), output))\n",
        "          \n",
        "          vout.write(frame)\n",
        "          \n",
        "          key = cv2.waitKey(1)\n",
        "          if key & 0xFF == ord('q'):\n",
        "              FPS = frames // (time.time() - start)\n",
        "              break\n",
        "          frames += 1\n",
        "          \n",
        "      else:\n",
        "          FPS = frames // (time.time() - start)\n",
        "          break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading network...\n",
            "Network successfully loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:179: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-300f172066fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pallete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwrite_video_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m           \u001b[0mvout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-300f172066fe>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pallete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwrite_video_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m           \u001b[0mvout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-09b0cb03fd62>\u001b[0m in \u001b[0;36mwrite_video_box\u001b[0;34m(x, results)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_PLAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'img' referenced before assignment"
          ]
        }
      ]
    }
  ]
}